{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5227634c-7a38-4aeb-a38e-84268ead8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9c262-4a2f-4987-a829-64090a7feab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting cwd with os, use if needed:\n",
    "# path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c2bab30-7269-4e11-bf56-ff44b21aaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './training'\n",
    "data_dir = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f71c633c-ff16-4376-8cd5-13e3f559811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing .DS_Store\n",
    "data_dir.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec7675ba-c449-459a-9c6a-3324442a7092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images of vegetarian\n",
      "Loading images of places\n",
      "Loading images of non_vegetarian\n"
     ]
    }
   ],
   "source": [
    "# Importing all images and labels for training:\n",
    "\n",
    "img_data_list = []\n",
    "labels = []\n",
    "\n",
    "for data in data_dir:\n",
    "    img_list = os.listdir(data_path+'/'+data)\n",
    "    print('Loading images of', data)\n",
    "    for img in img_list:\n",
    "        img_path = data_path +'/'+ data + '/' + img\n",
    "        try:\n",
    "            img = image.load_img(img_path, target_size= (28,28))\n",
    "            x = image.img_to_array(img)\n",
    "            x = preprocess_input(x)\n",
    "            img_data_list.append(x)\n",
    "            labels.append(data)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "307dd416-d13c-4ac2-aea4-74ed7c7a5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting images into a numpy array:\n",
    "img_data = np.array(img_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cfd5218-83e4-4d34-bbec-365205860563",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/train_sq_data.npy', img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c08c0e0e-c24b-4cba-bff2-e027a04819a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/train_sq_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4cde3578-4a59-466a-9303-b2b39baf5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.load('./data/train_sq_data.npy')\n",
    "labels = np.load('./data/train_sq_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fde7d25b-93e6-44f7-bef5-797da7e5fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use if needed\n",
    "# num_class = 3\n",
    "# num_of_samples = img_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda0407-beb2-4515-88b6-94cb1c5fe6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "39c10ab5-c0c5-4a19-a2c5-164f5e9c7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting labels into numerical values:\n",
    "\n",
    "numerical_labels = []\n",
    "for each in labels:\n",
    "    if each == 'vegetarian':\n",
    "        numerical_labels.append(0)\n",
    "    elif each == 'non_vegetarian':\n",
    "        numerical_labels.append(1)\n",
    "    else:\n",
    "        numerical_labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ace8346-0fe6-4fbd-aca4-f42d418c0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_labels= np.array(numerical_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9484486-ea8e-4e24-828d-0a856bde2ff4",
   "metadata": {},
   "source": [
    "# Train Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "913fab5d-82a2-46cd-9f46-c63debfb754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1446984-4151-455e-b5f9-6b9b28734c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = utils.to_categorical(numerical_labels, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0f869a89-a82c-4ae9-a019-06c0097a5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f78f59-16cb-431d-a474-b5bcdd52244c",
   "metadata": {},
   "source": [
    "# Modeling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7394136e-8bd8-4466-a690-64e04f75bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Complexity!\n",
    "# Instantiate a CNN\n",
    "cnn_model_2 = Sequential()\n",
    "\n",
    "# Add a convolutional layer\n",
    "cnn_model_2.add(Conv2D(filters=16,             # number of filters\n",
    "                       kernel_size=(3,3),      # height/width of filter\n",
    "                       activation='relu',      # activation function \n",
    "                       input_shape=(28,28,3))) # shape of input (image)\n",
    "\n",
    "# Add a pooling layer\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2))) # dimensions of region of pooling\n",
    "\n",
    "# Add another convolutional layer\n",
    "cnn_model_2.add(Conv2D(64,\n",
    "                       kernel_size=(3,3),\n",
    "                       activation='relu'))\n",
    "\n",
    "# Add another pooling layer\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# We have to remember to flatten to go from the \"box\" to the vertical line of nodes!\n",
    "cnn_model_2.add(Flatten())\n",
    "\n",
    "# Add a densely-connected layer with 64 neurons\n",
    "cnn_model_2.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add a densely-connected layer with 32 neurons\n",
    "cnn_model_2.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add a final layer with 3 neurons\n",
    "cnn_model_2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "cnn_model_2.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Fit model on training data\n",
    "history = cnn_model_2.fit(X_train,\n",
    "                          y_train,\n",
    "                          batch_size=128,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          epochs=10,\n",
    "                          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a2fa4-443e-4c13-a0a1-48c23df0d741",
   "metadata": {},
   "source": [
    "# Predicting Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8cd0025-fdc9-4f45-9e14-72b7aed905ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './testing'\n",
    "data_dir = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9932561-57e9-4adf-ab85-2691285c5cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e891eb9-6df2-4600-b80a-eaacecc7e655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ce106ca-da68-4544-9f60-b1884c7d94cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images of vegetarian\n",
      "Loading images of places\n",
      "Loading images of non_vegetarian\n"
     ]
    }
   ],
   "source": [
    "img_data_list = []\n",
    "labels = []\n",
    "image_paths = []\n",
    "\n",
    "for data in data_dir:\n",
    "    img_list = os.listdir(data_path+'/'+data)\n",
    "    print('Loading images of', data)\n",
    "    for img in img_list:\n",
    "        img_path = data_path+'/'+data + '/' +img\n",
    "        try:\n",
    "            img = image.load_img(img_path, target_size= (28,28))\n",
    "            x = image.img_to_array(img)\n",
    "            x = preprocess_input(x)\n",
    "            img_data_list.append(x)\n",
    "            labels.append(data)\n",
    "            image_paths.append(img_path)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2663b9ea-ea59-479b-b2a3-fa1ce9c8e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.array(img_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b610253e-b41c-4b6f-bd8e-cb46bc176a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/testing_sq_data.npy', img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4cd30f0-7a90-461f-a88e-74b8a32f3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/testing_sq_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3533be5c-dab2-426e-a7be-d1b9bdef42cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.load('./data/testing_sq_data.npy')\n",
    "labels = np.load('./data/testing_sq_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4099f8f0-605d-4be7-9a2e-3caaa30ae751",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 3\n",
    "num_of_samples = img_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0dd4e791-b4e9-414b-9c41-49f04cf91ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = []\n",
    "\n",
    "for each in labels:\n",
    "    if each == 'vegetarian':\n",
    "        num_labels.append(0)\n",
    "    elif each == 'non_vegetarian':\n",
    "        num_labels.append(1)\n",
    "    else:\n",
    "        num_labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45a602e9-0c4c-4109-b9e6-34b0e8132069",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels= np.array(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35913a69-e584-457d-9c73-d4778257e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eee3d806-33ec-4956-89a4-70e771b080ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = utils.to_categorical(num_labels, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00848f70-eacc-43f7-9a1f-4261b4996132",
   "metadata": {},
   "source": [
    "# Predicting on test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "865c34c8-243f-4833-9924-5f808eea7eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fb1447d90d0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67a193bd-b530-4fc8-b182-ba6c84768c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_vegetarian_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "074361e0-e61c-4b20-97ba-471bd8b1cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "places_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24b30d36-6507-44b7-b489-e6d5400ae1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vegetarian_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0ffa310f-ca3f-43d5-b97c-f167e25db4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, im in enumerate(x):\n",
    "    test_img = np.expand_dims(im, axis=0)\n",
    "    single_pred = cnn_model_2.predict(test_img,verbose=0)\n",
    "    if np.argmax(single_pred) == 0:\n",
    "        vegetarian_images.append(image_paths[i])\n",
    "    elif np.argmax(single_pred)==1:\n",
    "        non_vegetarian_images.append(image_paths[i])\n",
    "    else:\n",
    "        places_images.append(image_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea226ed8-78b6-486a-9523-015efba5b1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"./testing/vegetarian/Little Frank's Pizzeria+9myimage.jpg\",\n",
       " './testing/vegetarian/Benihana+7myimage.jpg',\n",
       " './testing/vegetarian/Indulge Diner+2myimage.jpg',\n",
       " './testing/vegetarian/Jade Siam+6myimage.jpg',\n",
       " './testing/vegetarian/La Fonda Latina+5myimage.jpg',\n",
       " './testing/vegetarian/Indulge Diner+7myimage.jpg',\n",
       " './testing/vegetarian/Bacio Pizzeria+7myimage.jpg',\n",
       " './testing/vegetarian/Cafe Rio Mexican Grill+6myimage.jpg',\n",
       " \"./testing/vegetarian/Angelo's Coal Oven Pizzeria+8myimage.jpg\",\n",
       " './testing/vegetarian/Bella Blu+8myimage.jpg',\n",
       " './testing/vegetarian/Baja Fresh+5myimage.jpg',\n",
       " './testing/vegetarian/3919555.jpg',\n",
       " './testing/vegetarian/Boqueria Soho+4myimage.jpg',\n",
       " './testing/vegetarian/Le Fat+9myimage.jpg',\n",
       " \"./testing/vegetarian/Hugo's Cellar+5myimage.jpg\",\n",
       " './testing/vegetarian/Cadillac Mexican Kitchen & Tequila Bar+8myimage.jpg',\n",
       " './testing/vegetarian/Marea+4myimage.jpg',\n",
       " './testing/vegetarian/Cipriani Downtown NYC+9myimage.jpg',\n",
       " \"./testing/vegetarian/Michael's+5myimage.jpg\",\n",
       " './testing/vegetarian/Chipotle Mexican Grill+2myimage.jpg',\n",
       " './testing/vegetarian/Chinatown Garden+4myimage.jpg',\n",
       " './testing/vegetarian/Cipriani Downtown NYC+1myimage.jpg',\n",
       " './testing/vegetarian/Gyro Time - Charleston+5myimage.jpg',\n",
       " './testing/vegetarian/Buona - Oak Lawn+6myimage.jpg',\n",
       " './testing/non_vegetarian/Yoshinoya (Slauson & Holmes) - Los Angeles+3myimage.jpg',\n",
       " './testing/non_vegetarian/Al Bawadi Grill+2myimage.jpg',\n",
       " './testing/non_vegetarian/Akimoto Sushi+9myimage.jpg',\n",
       " './testing/non_vegetarian/Allora+8myimage.jpg',\n",
       " \"./testing/non_vegetarian/Wendy's+8myimage.jpg\",\n",
       " './testing/non_vegetarian/Al Bawadi Grill+9myimage.jpg',\n",
       " './testing/non_vegetarian/Aloha Kitchen and Bar+1myimage.jpg']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vegetarian_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "33f3d0e0-3e98-43aa-b9d4-39fe404fb379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        9280      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                102464    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114,371\n",
      "Trainable params: 114,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model_2.save('cnn_model_2.h5')\n",
    "\n",
    "new_model = load_model('cnn_model_2.h5')\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d4406-b412-40d2-a5ff-9fb7099fafcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008440ac-7b6b-4550-ae02-c1ca4a962db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1d383adb-9eaf-4ccc-9856-4c5434ecb0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 2s 71ms/step - loss: 4.1669 - accuracy: 0.4775 - val_loss: 1.1887 - val_accuracy: 0.5473\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 1s 66ms/step - loss: 0.9829 - accuracy: 0.5915 - val_loss: 0.9427 - val_accuracy: 0.6171\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 1s 71ms/step - loss: 0.8439 - accuracy: 0.6486 - val_loss: 0.8623 - val_accuracy: 0.6481\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 2s 82ms/step - loss: 0.7099 - accuracy: 0.7149 - val_loss: 0.8454 - val_accuracy: 0.6682\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 2s 89ms/step - loss: 0.6694 - accuracy: 0.7358 - val_loss: 0.7934 - val_accuracy: 0.6899\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 2s 74ms/step - loss: 0.6083 - accuracy: 0.7603 - val_loss: 0.8116 - val_accuracy: 0.6822\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 1s 70ms/step - loss: 0.5862 - accuracy: 0.7611 - val_loss: 0.7614 - val_accuracy: 0.6930\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 2s 88ms/step - loss: 0.5606 - accuracy: 0.7789 - val_loss: 0.7331 - val_accuracy: 0.7008\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 2s 90ms/step - loss: 0.5068 - accuracy: 0.7929 - val_loss: 0.7452 - val_accuracy: 0.6961\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 2s 77ms/step - loss: 0.4866 - accuracy: 0.8119 - val_loss: 0.8501 - val_accuracy: 0.6806\n"
     ]
    }
   ],
   "source": [
    "# More Complexity!\n",
    "# Instantiate a CNN\n",
    "cnn_model_3 = Sequential()\n",
    "\n",
    "# Add a convolutional layer\n",
    "cnn_model_3.add(Conv2D(filters=64,             # number of filters\n",
    "                       kernel_size=(3,3),      # height/width of filter\n",
    "                       activation='relu',      # activation function \n",
    "                       input_shape=(28,28,3))) # shape of input (image)\n",
    "\n",
    "# Add a pooling layer\n",
    "cnn_model_3.add(MaxPooling2D(pool_size=(2,2))) # dimensions of region of pooling\n",
    "\n",
    "# Add another convolutional layer\n",
    "cnn_model_3.add(Conv2D(32,\n",
    "                       kernel_size=(3,3),\n",
    "                       activation='relu'))\n",
    "\n",
    "# Add another pooling layer\n",
    "cnn_model_3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# We have to remember to flatten to go from the \"box\" to the vertical line of nodes!\n",
    "cnn_model_3.add(Flatten())\n",
    "\n",
    "# Add a densely-connected layer with 64 neurons\n",
    "cnn_model_3.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add a densely-connected layer with 32 neurons\n",
    "cnn_model_3.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add a final layer with 3 neurons\n",
    "cnn_model_3.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "cnn_model_3.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Fit model on training data\n",
    "history = cnn_model_3.fit(X_train,\n",
    "                          y_train,\n",
    "                          batch_size=128,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          epochs=10,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec76d8a-457d-4a4c-8486-b73ec3fabb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "200, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2ffa2-34b1-49d1-92ac-ddf3b15ccf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1439c-00d0-4317-8a50-f47dfd7ba9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
